{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# crawling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9wXz_mWduqr",
        "outputId": "7b87d50f-0841-4412-ed5c-d0d2602fe4b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sprynger in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sprynger) (5.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from sprynger) (2.32.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.12/dist-packages (from sprynger) (2.5.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from sprynger) (4.3.8)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->sprynger) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->sprynger) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->sprynger) (2025.8.3)\n"
          ]
        }
      ],
      "source": [
        "pip install sprynger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bHjS8uedUyN",
        "outputId": "c42d1f56-edd3-45b6-ad83-dbc104c55532"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyword: web mining\n",
            "Total hasil: 317019\n",
            "\n",
            "Keyword: web usage mining\n",
            "Total hasil: 90635\n",
            "\n",
            "Keyword: data mining\n",
            "Total hasil: 1169861\n",
            "\n",
            "Hasil crawling sudah disimpan ke springer_crawling.csv\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# Silahkan buat API key dari https://dev.springernature.com/#api\n",
        "API_KEY = \"d16e27ffd519f216eff78d914d5aae4c\"\n",
        "\n",
        "# Variasi kata kunci\n",
        "keywords = [\"web mining\", \"web usage mining\", \"data mining\"]\n",
        "\n",
        "# Endpoint API Springer\n",
        "url = \"https://api.springernature.com/meta/v2/json\"\n",
        "\n",
        "all_records = []\n",
        "\n",
        "for kw in keywords:\n",
        "    params = {\n",
        "        \"q\": kw,        # query keyword\n",
        "        \"api_key\": API_KEY,\n",
        "        \"p\": 20         # jumlah hasil per request (max 50)\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, params=params)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        print(f\"Keyword: {kw}\")\n",
        "        print(f\"Total hasil: {data['result'][0]['total']}\\n\")\n",
        "\n",
        "        for record in data.get('records', []):\n",
        "            all_records.append({\n",
        "                \"keyword\": kw,\n",
        "                \"doi\": record.get('doi', 'N/A'),\n",
        "                \"title\": record.get('title', 'No title'),\n",
        "                \"publicationName\": record.get('publicationName', 'N/A'),\n",
        "                \"url\": record.get('url', [{'value': 'N/A'}])[0]['value']\n",
        "            })\n",
        "    else:\n",
        "        print(\"Error:\", response.status_code, response.text)\n",
        "\n",
        "# Simpan ke CSV\n",
        "df = pd.DataFrame(all_records)\n",
        "df.to_csv(\"springer_crawling.csv\", index=False, encoding=\"utf-8\")\n",
        "print(\"Hasil crawling sudah disimpan ke springer_crawling.csv\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
