{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# keywordExtraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JywzkqNguMLK",
        "outputId": "15aee3ad-1b34-46af-9bb9-cdc53d85b255"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.4.2)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (6.3.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy) (0.2.14)\n",
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.12/dist-packages (0.8.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "! pip install pypdf\n",
        "! pip install ftfy\n",
        "! pip install pyspellchecker\n",
        "! pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESAnisXEuWOr",
        "outputId": "a87b1a25-8252-4152-953b-e3441a337eeb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from pypdf import PdfReader\n",
        "import ftfy\n",
        "import unicodedata\n",
        "from spellchecker import SpellChecker\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import networkx as nx\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
        "# --- PERSIAPAN ---\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "#  --- KONFIGURASI NLTK ---\n",
        "# Memastikan resource tokenizer tersedia\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    print(\"[INFO] Mengunduh data NLTK...\")\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXB5b6rquizB"
      },
      "source": [
        "## 1. Load data </br>\n",
        "1. membaca data pdf </br>\n",
        "2. menghapus kata kata noise, hapus baris yang terlalu pendek<br>\n",
        "3. memotong refference/daftar pustaka<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CVQWSvMPuX-a"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Membaca PDF, lalu memperbaiki encoding (ftfy) dan normalisasi Unicode.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        reader = PdfReader(pdf_path)\n",
        "        full_text = \"\"\n",
        "        for page in reader.pages:\n",
        "            text = page.extract_text()\n",
        "            if text:\n",
        "                # 1. FTFY: Perbaiki encoding teks yang rusak (mojibake)\n",
        "                # Ini memperbaiki karakter aneh sebelum diproses lebih lanjut\n",
        "                text = ftfy.fix_text(text)\n",
        "\n",
        "                # 2. Unicode Normalization: Memecah ligatures (ﬁ -> f i)\n",
        "                text = unicodedata.normalize('NFKD', text)\n",
        "\n",
        "                full_text += text + \"\\n\"\n",
        "        return full_text\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Gagal membaca PDF: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def clean_journal_text(raw_text):\n",
        "    \"\"\"\n",
        "    Membersihkan noise, header/footer, dan memotong bagian referensi.\n",
        "    \"\"\"\n",
        "    if not raw_text:\n",
        "        return \"\"\n",
        "\n",
        "    # 1. Normalisasi Awal\n",
        "    text = raw_text.strip()\n",
        "\n",
        "    # --- TAHAP 1: MENGHAPUS NOISE UMUM (REGEX) ---\n",
        "    # Menghapus citasi angka standar ex: [1], [1, 2], [1-3]\n",
        "    text = re.sub(r'\\[[\\d,\\s\\-]+\\]', '', text)\n",
        "    # Menghapus URL dan DOI\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'doi: \\S+', '', text, flags=re.IGNORECASE)\n",
        "    # Menghapus simbol LaTeX/Matematika ($...$)\n",
        "    text = re.sub(r'\\$.*?\\$', '', text)\n",
        "\n",
        "    # --- TAHAP 2: MENGHAPUS HEADER/FOOTER & METADATA PER BARIS ---\n",
        "    lines = text.split('\\n')\n",
        "    cleaned_lines = []\n",
        "\n",
        "    # Daftar pola noise di header/footer (Sesuaikan jika ada pola lain)\n",
        "    noise_patterns = [\n",
        "        r'^\\d+\\s*of\\s*\\d+$',         # Halaman: \"1 of 24\"\n",
        "        r'^\\d+$',                    # Halaman: Angka saja \"12\"\n",
        "        r'Diagnostics 2024',         # Nama Jurnal/Tahun\n",
        "        r'Authorized licensed use',  # License info\n",
        "        r'Downloaded on',            # Timestamp download\n",
        "        r'Creative Commons',         # Copyright\n",
        "        r'©',                        # Simbol copyright\n",
        "        r'IEEE Xplore',\n",
        "        r'MDPI',\n",
        "        r'Vol\\.\\s*\\d+',              # Volume info\n",
        "        r'ISSN\\s*\\d+',               # ISSN\n",
        "    ]\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        is_noise = False\n",
        "\n",
        "        # Cek pola noise\n",
        "        for pattern in noise_patterns:\n",
        "            if re.search(pattern, line, re.IGNORECASE):\n",
        "                is_noise = True\n",
        "                break\n",
        "\n",
        "        # Hapus baris yang terlalu pendek (sisa potongan) kecuali Judul Bab (Huruf Besar)\n",
        "        if len(line) < 4 and not re.match(r'^[A-Z0-9\\.]+$', line):\n",
        "            is_noise = True\n",
        "\n",
        "        if not is_noise:\n",
        "            cleaned_lines.append(line)\n",
        "\n",
        "    # Gabungkan kembali baris yang sudah bersih\n",
        "    text = '\\n'.join(cleaned_lines)\n",
        "\n",
        "    # --- TAHAP 3: STRUKTURISASI & PERBAIKAN KATA ---\n",
        "\n",
        "    # a. Mencari posisi awal (Abstract)\n",
        "    # Catatan: Jika ingin mengambil Judul paling atas, matikan bagian ini.\n",
        "    # Namun, biasanya bagian atas penuh dengan nama penulis/afiliasi yang sulit dibersihkan.\n",
        "    start_match = re.search(r'(Abstract|Abstrak)[-—:.]?', text, re.IGNORECASE)\n",
        "    if start_match:\n",
        "        # Ambil teks mulai dari kata \"Abstract\"\n",
        "        text = text[start_match.start():]\n",
        "\n",
        "    # b. Perbaiki kata terpotong (Hyphenation) sebelum tokenisasi\n",
        "    # Contoh: \"process-\\ning\" menjadi \"processing\"\n",
        "    text = re.sub(r'-\\n\\s*', '', text)\n",
        "\n",
        "    # c. Jadikan satu baris panjang (Single paragraph flow)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # --- TAHAP 4: MEMOTONG BAGIAN AKHIR (REFERENCES) MENGGUNAKAN NLTK ---\n",
        "\n",
        "    # Pecah teks menjadi daftar kalimat\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "    # Pola Regex untuk mendeteksi Header Bagian Akhir\n",
        "    header_pattern = r'^\\s*(?:\\d+\\.?\\s*)?(References|Daftar Pustaka|Bibliography|Acknowledgments|Ucapan Terima Kasih|Funding|Conflicts of Interest|Rujukan)\\b'\n",
        "\n",
        "    cutoff_index = -1\n",
        "\n",
        "    # Loop setiap kalimat\n",
        "    for i, sent in enumerate(sentences):\n",
        "        # Cek kalimat pendek yang mengandung kata kunci header\n",
        "        if len(sent) < 1000 and re.search(header_pattern, sent, re.IGNORECASE):\n",
        "            print(f\"[LOG] Memotong teks mulai dari: '{sent}'\")\n",
        "            cutoff_index = i\n",
        "            break # Stop di temuan pertama\n",
        "        else:\n",
        "          print(\"tidak ditemukan\")\n",
        "\n",
        "    # Potong list kalimat\n",
        "    if cutoff_index != -1:\n",
        "        final_sentences = sentences[:cutoff_index]\n",
        "    else:\n",
        "        final_sentences = sentences\n",
        "\n",
        "    # Gabungkan kembali\n",
        "    return ' '.join(final_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8OtcxrCvQBN",
        "outputId": "49ae6bcc-871f-41cf-a154-b9ff803367be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sedang memproses: /content/drive/MyDrive/Semester 7/PPW/UAS_PROJEK/Classifying_Alzheimer_Disease_using_VGG19.pdf ...\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "tidak ditemukan\n",
            "[LOG] Memotong teks mulai dari: 'REFERENCES H. S. Zaina, S. Brahim Belhaouari, T. Stanko, and V. Gorovoy, \"An Exemplar Pyramid Feature Extraction Based Alzheimer Disease 2022, R. Sharma and H. K. Meena, \"Utilizing graph Fourier transform for automatic Alzheime r's disease detection from EEG signals,\" Int.'\n",
            "\n",
            "========================================\n",
            "HASIL PREPROCESSING BERSIH\n",
            "========================================\n",
            "Abstract— A disease called as Alzheimer, always put health agencies in alarming situation and thus necessitating targeted interventions. This study employs the VGG19 convolutional neural network (CNN) in conjunction with the ADNI dataset to enhance the precision of Alzheimer's detection. The ADNI dataset, a comprehensive repository of neuroimaging data spanning various cognitive states, serves as the foundation for robust Alzheimer's detection. Leveraging the VGG19 CNN architecture renowned for its image classification capabilities, we analyze three-dimensional MRI scans to discern subtle patterns indicative of Alzheimer 's disease. Through fine-tuning and transfer learning, our research adapts VGG19 to accurately detect Alzheimer's disease and its several subclasses. Further this study tries to illuminate rapid progression of the concerned disease. Despite challenges such as Alzheimer's complexity, high-dimensional MRI data, and ethical considerations, our findings represent a signifi...\n",
            "(dan seterusnya)\n",
            "\n",
            "[SUKSES] Hasil bersih disimpan ke 'hasil_bersih2.txt'\n"
          ]
        }
      ],
      "source": [
        "input_pdf = \"/content/drive/MyDrive/Semester 7/PPW/UAS_PROJEK/Classifying_Alzheimer_Disease_using_VGG19.pdf\"\n",
        "print(f\"Sedang memproses: {input_pdf} ...\")\n",
        "# 1. Ekstrak teks mentah dari PDF\n",
        "raw_text = extract_text_from_pdf(input_pdf)\n",
        "\n",
        "if raw_text:\n",
        "    # 2. Bersihkan teks\n",
        "    clean_text = clean_journal_text(raw_text)\n",
        "\n",
        "    # 3. Tampilkan atau Simpan Hasil\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(\"HASIL PREPROCESSING BERSIH\")\n",
        "    print(\"=\"*40)\n",
        "    print(clean_text[:1000] + \"...\\n(dan seterusnya)\") # Print 1000 karakter pertama\n",
        "\n",
        "    # Opsi: Simpan ke file teks\n",
        "    output_filename = \"hasil_bersih2.txt\"\n",
        "    with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(clean_text)\n",
        "    print(f\"\\n[SUKSES] Hasil bersih disimpan ke '{output_filename}'\")\n",
        "else:\n",
        "    print(\"[GAGAL] Tidak ada teks yang bisa diekstrak.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0y88gs_1vy7v"
      },
      "source": [
        "### Cleaning lanjutan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "lGuOsYnhvmdT"
      },
      "outputs": [],
      "source": [
        "def advanced_preprocessing(text):\n",
        "    \"\"\"\n",
        "    Melakukan pembersihan mendalam khusus untuk keperluan Keyword Extraction.\n",
        "    Fitur:\n",
        "    1. Hapus sitasi (Author, Year)\n",
        "    2. Hapus email/URL\n",
        "    3. Hapus kalimat kotor (banyak angka/simbol)\n",
        "    4. Hapus kalimat terlalu pendek\n",
        "    \"\"\"\n",
        "\n",
        "    # --- TAHAP 1: PEMBERSIHAN POLA REGEX (LEVEL TEKS) ---\n",
        "\n",
        "    # 1. Hapus Email\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)\n",
        "\n",
        "    # 2. Hapus URL (www, http, https)\n",
        "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
        "\n",
        "    # 3. Hapus Sitasi format (Name et al., 2012) atau (Name, 2012)\n",
        "    # Logika: Cari kurung yang diakhiri dengan 4 digit angka (tahun)\n",
        "    text = re.sub(r'\\([^)]*?\\d{4}\\)', '', text)\n",
        "\n",
        "    # 4. Hapus referensi [1], [12] (backup jika tahap sebelumnya lolos)\n",
        "    text = re.sub(r'\\[\\d+(?:,\\s*\\d+)*\\]', '', text)\n",
        "\n",
        "    # 5. Hapus kata-kata sisa referensi umum\n",
        "    text = re.sub(r'\\b(et al\\.|Ibid\\.|Vol\\.|pp\\.|Fig\\.|Table)\\b', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # --- TAHAP 2: FILTERING PER KALIMAT ---\n",
        "\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    clean_sentences = []\n",
        "\n",
        "    for sent in sentences:\n",
        "        # A. Hapus kalimat yang terlalu pendek (Ide Anda)\n",
        "        # Kita hitung jumlah kata (token)\n",
        "        words = sent.split()\n",
        "        if len(words) < 5:\n",
        "            continue # Skip kalimat ini\n",
        "\n",
        "        # B. Hapus kalimat \"Matematika/Tabel\" (Ide Anda)\n",
        "        # Logika: Jika jumlah angka dan simbol > 30% dari total karakter kalimat, buang.\n",
        "        num_digits = len(re.findall(r'\\d', sent))\n",
        "        num_symbols = len(re.findall(r'[+\\-*/%=<>±]', sent)) # Simbol math\n",
        "        total_chars = len(sent.replace(\" \", \"\"))\n",
        "\n",
        "        if total_chars > 0:\n",
        "            ratio_noise = (num_digits + num_symbols) / total_chars\n",
        "            if ratio_noise > 0.25: # Jika lebih dari 25% isinya angka/simbol\n",
        "                continue # Skip kalimat ini (kemungkinan besar tabel atau rumus)\n",
        "\n",
        "        # C. Pembersihan akhir per kalimat (Spasi ganda & trim)\n",
        "        sent = re.sub(r'\\s+', ' ', sent).strip()\n",
        "\n",
        "        clean_sentences.append(sent)\n",
        "\n",
        "    # Gabungkan kembali\n",
        "    final_text = ' '.join(clean_sentences)\n",
        "    return final_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0Ci-BPRKwD3Q"
      },
      "outputs": [],
      "source": [
        "clean_result = advanced_preprocessing(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gDu90C7Nw_8h"
      },
      "outputs": [],
      "source": [
        "def final_polishing(text):\n",
        "    \"\"\"\n",
        "    TAHAP 3 (FINAL): Menghapus ANGKA dan SEMUA SIMBOL TANDA BACA.\n",
        "    Hanya menyisakan huruf (a-z, A-Z) dan spasi.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # 1. KHUSUS APOSTROPHE: Hapus (ganti dengan empty string) agar nyambung\n",
        "    # Kita handle tanda kutip lurus (') dan kutip miring/smart quote (’) yang sering ada di PDF\n",
        "    text = re.sub(r\"['’]\", \"\", text)\n",
        "    # 1. Hapus Angka (0-9)\n",
        "    # Contoh: \"VGG19\" -> \"VGG\", \"2024\" -> \"\"\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # 2. Hapus semua simbol & tanda baca (selain huruf dan spasi)\n",
        "    # Regex [^a-zA-Z\\s] berarti: \"Hapus karakter apa pun KECUALI huruf dan spasi\"\n",
        "    # Kita ganti dengan spasi ' ' agar kata tidak menempel (word1/word2 -> word1 word2)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
        "\n",
        "    # 3. Normalisasi Spasi (hapus double space akibat penghapusan simbol)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "oPegDINwxMJs"
      },
      "outputs": [],
      "source": [
        "final_cleaned_text = final_polishing(clean_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "debriUsoxg9i"
      },
      "source": [
        "### Speechecker\n",
        "memperbaiki kata yang typo atau terpisah dengan spasi akibat cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "A3WG7RCsxRED"
      },
      "outputs": [],
      "source": [
        "def smart_word_merge(text):\n",
        "    \"\"\"\n",
        "    Menggabungkan kata yang terputus (misal: 'classi cation' -> 'classification')\n",
        "    dengan mengecek validitas kata gabungan di kamus bahasa Inggris.\n",
        "    \"\"\"\n",
        "    spell = SpellChecker()\n",
        "\n",
        "    # 1. Tokenisasi sederhana (pisahkan berdasarkan spasi)\n",
        "    words = text.split()\n",
        "    merged_words = []\n",
        "    skip_next = False\n",
        "\n",
        "    for i in range(len(words)):\n",
        "        if skip_next:\n",
        "            skip_next = False\n",
        "            continue\n",
        "\n",
        "        current_word = words[i]\n",
        "\n",
        "        # Cek apakah ini kata terakhir\n",
        "        if i + 1 < len(words):\n",
        "            next_word = words[i+1]\n",
        "\n",
        "            # Bersihkan tanda baca untuk pengecekan kamus\n",
        "            # Kita hanya ingin cek inti katanya (misal: \"classi\" dan \"fication.\")\n",
        "            curr_clean = re.sub(r'[^a-zA-Z]', '', current_word)\n",
        "            next_clean = re.sub(r'[^a-zA-Z]', '', next_word)\n",
        "\n",
        "            combined_clean = curr_clean + next_clean\n",
        "\n",
        "            # --- LOGIKA UTAMA ---\n",
        "            # Jika kata gabungan (combined) ADA di kamus (known)\n",
        "            # DAN kata saat ini (current) TIDAK ADA di kamus (unknown/typo)\n",
        "            # Maka kemungkinan besar itu adalah kata yang putus.\n",
        "\n",
        "            if (combined_clean in spell.known([combined_clean])) and \\\n",
        "               (curr_clean not in spell.known([curr_clean])):\n",
        "\n",
        "                # Gabungkan!\n",
        "                # Kita pakai combined_clean agar bersih, atau gabung raw stringnya\n",
        "                # Disini kita gabung raw string tapi hapus spasi diantaranya\n",
        "                print(f\"[AUTO-FIX] Menggabungkan: '{current_word}' + '{next_word}' -> '{combined_clean}'\")\n",
        "                merged_words.append(combined_clean)\n",
        "                skip_next = True # Karena kata berikutnya sudah kita makan\n",
        "                continue\n",
        "\n",
        "        merged_words.append(current_word)\n",
        "\n",
        "    return \" \".join(merged_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylWakvVQxtTF",
        "outputId": "c50b1f07-c414-41e7-8a23-7b357adf9c12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[AUTO-FIX] Menggabungkan: 'commun' + 'ities' -> 'communities'\n",
            "[AUTO-FIX] Menggabungkan: 'rapi' + 'd' -> 'rapid'\n",
            "[AUTO-FIX] Menggabungkan: 'diagnosi' + 'ng' -> 'diagnosing'\n",
            "[AUTO-FIX] Menggabungkan: 'brai' + 'n' -> 'brain'\n",
            "[AUTO-FIX] Menggabungkan: 'probabilis' + 'tic' -> 'probabilistic'\n",
            "[AUTO-FIX] Menggabungkan: 'learni' + 'ng' -> 'learning'\n",
            "[AUTO-FIX] Menggabungkan: 'proba' + 'bilities' -> 'probabilities'\n",
            "[AUTO-FIX] Menggabungkan: 'indica' + 'tes' -> 'indicates'\n"
          ]
        }
      ],
      "source": [
        "pengecekan_kata = smart_word_merge(final_cleaned_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6ps6bG0yEzN"
      },
      "source": [
        "### Preprocessing Stopworld"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "SXp4EGgLxxvB"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "def remove_stopwords(text):\n",
        "    \"\"\"\n",
        "    TAHAP TERAKHIR: Menghapus Stopwords (kata umum tidak bermakna).\n",
        "    \"\"\"\n",
        "    # 1. Ambil daftar stopwords bahasa Inggris\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Tambahkan stopword custom spesifik jurnal jika perlu\n",
        "    custom_stops = {'et', 'al', 'study', 'based', 'using', 'proposed', 'method', 'paper'}\n",
        "    stop_words.update(custom_stops)\n",
        "\n",
        "    # 2. Tokenisasi kata\n",
        "    words = text.split()\n",
        "\n",
        "    # 3. Filter\n",
        "    filtered_words = []\n",
        "    for w in words:\n",
        "        # Ubah ke lowercase untuk pengecekan stopword\n",
        "        w_lower = w.lower()\n",
        "\n",
        "        # Syarat masuk:\n",
        "        # - Bukan stopword\n",
        "        # - Panjang kata > 2 huruf (menghindari sisa-sisa 'x', 'y', 'de')\n",
        "        if w_lower not in stop_words and len(w_lower) > 2:\n",
        "            filtered_words.append(w) # Simpan kata aslinya (boleh w atau w_lower)\n",
        "\n",
        "    return \" \".join(filtered_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaDnyZO8yQuK",
        "outputId": "315157f2-7bf7-4130-e922-ee6a29b9ebcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SUKSES] Final cleaned text disimpan ke 'Abstract disease called Alzheimer always put health agencies alarming situation thus necessitating targeted interventions employs VGG convolutional neural network CNN conjunction ADNI dataset enhance precision Alzheimers detection ADNI dataset comprehensive repository neuroimaging data spanning various cognitive states serves foundation robust Alzheimers detection Leveraging VGG CNN architecture renowned image classification capabilities analyze three dimensional MRI scans discern subtle patterns indicative Alzheimer disease fine tuning transfer learning research adapts VGG accurately detect Alzheimers disease several subclasses tries illuminate rapid progression concerned disease Despite challenges Alzheimers complexity high dimensional MRI data ethical considerations findings represent significant advancement Alzheimers disease identification integrating technology medicine research enhances diagnostic accuracy also contributes broader understanding Alzheimers disease Keywords Visual Geometry Group Alzheimer Disease Cognitive Impairments Alzheimer Disease Neuroimaging Initiative INTRODUCTION Alzheimer becomes common type dementia nowadays collection symptoms known dementia progressive chronic nature impairing ones capacity comprehend information ideas beyond typical ageing Dementia impairs thinking memory orientation learning ability calculation language problems understanding judging Control emotions social activity behavior motivation cognitive functions often linked need impairment cognitive function found approximately million people worldwide suffering Dementia according report among belongs middle low salaried families count estimated rise million report million Sudden growth cases significantly affects individuals health relatives communities effects Dementia ychological physical social symptoms Dementia classified three stages Early stage first years Middle Staged considered second years Late stage years recent works Alzheimers disease detection EEG signals done pathophysiology Alzheimer directly linked injury sometimes death neurons several parts brain starts hippocampus area brain responsible learning memory moves remaining parts brain era economic growth advancement computer technology doctors need rapid accurate diagnostic methods diagnose treat disease get rid said disease patients contrast traditional methods disease diagnosis conventional methods diagnosing disease patients undergo different levels severity prior declared disease However may delayed diagnosing earlier patient condition becomes serious late stage disease Early diagnosis extremely important patients helps take precautionary measures helps clinicians identify risk progression also helps patients understand seriousness disease take preventive measures including lifestyle changes drugs prevalence Alzheimers disease continues rise effects individuals society economies becoming increasingly apparent handle issue proposing novel methodologies Machine Learning Deep Learning employed neurologists detect Alzheimer disease early stages prevent progression late stages particularly context limited availability trained personnel rapid rise number individuals Alzheimers disease Neuroimaging technologies include several methods deep learning common efficient way diagnosing categorising disorders large datasets However current work performs poorly data sets noise problem overfitting requires training time computational effort proposes VGG prediction approach MRI image takes less time train provides better accuracy RELATED WORK Zaina used MLPs mostly used tabular data lack inherent ability capture spatial information present neuroimaging data MRI scans literature various segmentation techniques used analyze image data Morphological opeartors used authors Angela Lombardi used SHAP Shapley Addictive Explanations algorithm employed elucidate choices made random forest didnt give much thought explainable machine learning models could make results harder understand believe Fuadah used Alexnet According result model showed increasing value learning rate may cause raising value loss named overshooting Nitika Goenka used MIRIAD dataset DCNN framework include details size features dataset research offer understanding regarding practicality framework generalizing datasets populations practical use Regularised Extreme Learning Machine RELM Import Vec tor Machine IVM Support Vector Machine SVM evaluated compared performance ADNI dataset conducted Sudharsan colleagues However early detection Alzheimers utilizing different classification strategies challenging Ramzan used ResNet increased processing time noises neuro images remove noises used high pass filter InNoemi Massetti used Random Forest ADNI dataset algorithm efficaciously foretold non converting ncMCI subjects Emina Alickovic used Random Forest outcomes trials demonstrate Random Forest tell difference control people accuracy Khvostikov used CNN sMRI DTI dataset imbalance datasets root cause overfitting problem caused fact various classes varying capabilities Kazemi used Alexnet fMRI remove noises neuro images Spatial smoothing used Gaussian kernel high pass filter used Yang colleagues used autoencoder first followed Convolutional Neural Network CNN extract features separately attributes combined completely connected layer thorough increases time consumption computational complexity due large number features extracted initiatives designed improve sickness detection treatment ultimate goal supporting perhaps saving patient lives researcher described patients experience multiple phases receiving diagnosis contrast traditional diagnostic techniques Due varying diagnosis phases may take longer patient may reach later stage illness order help patients take preventative action help doctors determine risk progression early diagnosis crucial Additionally supports people realizing severity condition implementing preventative measures medication lifestyle modifications Alzheimers disease becoming common comes consequences people communities economies AbdulAzeem used deep learning algorithms particularly Convo lutional Neural Networks CNN classification Alzheimers disease MRI scans highlight current gap effective disease modifying treatments Alzheimers disease also pointing hopeful role fluid biomarkers could play believe indicators potential help design effective medications pave way personalised approach treatment provides comprehensive literature review techniques early Alzheimers Disease detection brain image datasets employing machine learning algorithms leveraging smartphone sensors computer hardware software tools ultimately achieving successful detection classification probabilistic atlas salient points images detecting classifying hippocampal structural changes biomarker Alzheimers disease achieving accuracy AUC detecting mild cases among cognitively normal individuals aged Islam colleagues offer revolutionary deep learning model detecting classifying Alzheimers disease across many classes Brain MRI data model makes use advanced deep convolutional networks effectiveness demonstrated Open Access Series Imaging Studies OASIS database Many researchers also contributed various applications disease prediction machine learning OBJECTIVE RESEARCH WORK proposal seeks detect categorise stages Alzheimers disease people aged older goal accurately distinguish five stages Alzheimers progression Cognitive Normal Early Mild Cognitive Impairment Late Mild Cognitive Impairment Mild Cognitive Impairment Alzheimers Disease data Alzheimers Disease Neuroimaging Initiative ADNI dataset project aims make major advances medical diagnoses combining VGG model huge ADNI ataset aims provide reliable technique early detection Alzheimers disease increasing ssibility better care outcomes patients METHODOLOGY Figure depicts step system architecture uses VGG detect Alzheimer Disease shown Fig utilizes ADNI dataset Structural MRI Images resized classification disease VGG different classes EMCI LMCI MCI done description steps involved detecting Alzheimer Disease follows Data Preparation Collect ADNI dataset containing neuroimaging data preprocessed dataset Preprocess data Standardize image dimensions Data Split Training Testing Validation Feature Extraction VGG Load pre trained VGG architecture Deriving key characteristics neuroimaging data VGG convolution max pooling layers Custom Classification Dense Layers output layer activated softmax activation class probabilities Model Training Evaluation Train model training data monitoring validation performance Evaluate models accuracy precision recall score test set Deploy trained model Alzheimers disease detection Fig Chronological Order Disease Detection Input MRI Image ADNI dataset preprocessed Output Classification Alzheimer Disease classes LMCI EMCI MCI Step Import required libraries insert dataset Step Divide dataset train test data Step Pre process dataset Resize images Step Load VGG pre trained model input dimension Iterate layers VGG transfer learning trainable property false Create flattened layer takes features derived VGG model input Add fully connected output layer prediction classes softmax activation function Create new model deep learning frameworks function takes VGG input outputs Display extraction models architecture Step Validate data check accuracy save model file Fig VGG model Alzheimers Detection Figure depicts process used VGG model feature extraction classification Alzheimer Disease Classes EMCI LMCI MCI Module Convolutional yers Conv Conv layers made filters Fig MaxPooling done sliding window MRI image orientation stride Module Convolutional Layers Conv Conv module consists Conv Conv layers composed filters stage complex features learned Additionally MaxPool layer reduces spatial dimensions Fig Module Convolutional Layers Conv Conv Conv Convolutional layers composed filters layer captures features represent structures MRI images module max pooling layer spatial reduction layer extracts higher level features Fig Module Convolutional Layers Conv Conv Conv Convolutional layers composed filters layer captures intricate structure details module max pooling layer spatial reduction layer retailing essential information Fig Module Convolutional Layers Conv Conv Conv Convolutional layers composed filters layer captures high level structure details module max pooling layer spatial reduction layer Fig Flattening passing module convolution max pooling layers features extracted flattened array size input fully connected Layers Fig Fully Connected Layer Dense Layers bottom network usually three dense layers connected together prediction output layer three dense layers responsible taking highlevel data making predictions neurons respectively Fig Output Layer output layer produces class probabilities detect Alzheimers disease number output neurons viz EMCI LMCI MCI Softmax activation often convert networks output probabilities Fig System Performance Assessment assessed system performance metrics accuracy recall precision processes computing performance indicators detailed accordingly Accuracy Precision Recall FScore context represents true positive outcomes denotes true negative outcomes Similarly correspond false positive false negative outcomes respectively models training accuracy loss depicted Fig shows increase accuracy ere decrease loss shows overfitting problem looked MRI images Alzheimers cognitive normal mild early mild late mild cognitive impairment utilized images training data images test data images trained VGG Adam optimizer measured functioning metrics accuracy recall accuracy precision score model VGG recognize EMCI LMCI conditions accuracy loss demonstrates confusion matrix true labels mod els predictions MRI images tested MRI classified correctly indicates Alzheimers Disease indicates Cogni tive Normal indicates Early Mild Cognitive Impairment EMCI indicates Late Mild Cognitive Impairment LMCI indicates Mild Cognitive Impairment MCI classes axis respectively Figure depicts precision recall scores within range indicating low level error indicates Alzheimers Disease indicates Cognitive Normal indicates Early Mild Cognitive Impairment EMCI indicates Late Mild Cognitive Impairment LMCI indicates Mild Cognitive Impairment MCI respectively Fig Training Validation Accuracy Loss epoch Fig Testing Data Confusion Matrix Fig Performance VGG model shows comparative analysis CONCLUSION research employed Convolutional Neural Network CNN leveraging VGG architecture diagnose Alzheimers disease advanced deep learning methodologies combining learning neuroimaging data researchers aimed etect signs condition help medical professionals understand disease better improve lives living experiment showed Adam optimizer model classify MRI Alzheimers datasets Alzheimer disease cognitive normal early mild late mild mild cognitive impairment conditions accuracy loss future use cutting edge deep learning methods different datasets combine one make prediction efficient effective earlier stages also use ensemble models get different deep learning architectures like VGG ResNet Inception get best results'\n"
          ]
        }
      ],
      "source": [
        "stopwords = remove_stopwords(pengecekan_kata)\n",
        "\n",
        "output_final= \"CleaningFInal.txt\"\n",
        "with open(output_final, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(stopwords)\n",
        "print(f\"[SUKSES] Final cleaned text disimpan ke '{stopwords}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4Obp9yVyXs1"
      },
      "source": [
        "### Preprocessing Final\n",
        "1. Lemizitation<br>\n",
        "2. bigram<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "uyiB39tSyTt-"
      },
      "outputs": [],
      "source": [
        "def advanced_token_processing(text_bersih):\n",
        "    \"\"\"\n",
        "    PIPELINE UTAMA:\n",
        "    1. Tokenisasi\n",
        "    2. Lemmatization (networkS -> network)\n",
        "    3. Bigram Formation (neural + network -> neural_network)\n",
        "    \"\"\"\n",
        "    # 1. TOKENISASI\n",
        "    tokens = text_bersih.split()\n",
        "\n",
        "    # 2. LEMMATIZATION (Dilakukan SEBELUM Bigram)\n",
        "    # Tujuannya agar bentuk jamak (plural) menjadi tunggal dulu\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemma_tokens = []\n",
        "    for token in tokens:\n",
        "        # Lemmatize noun (kata benda)\n",
        "        lemma = lemmatizer.lemmatize(token, pos='n')\n",
        "        # Lemmatize verb (kata kerja) - opsional tapi bagus untuk jurnal\n",
        "        lemma = lemmatizer.lemmatize(lemma, pos='v')\n",
        "        lemma_tokens.append(lemma)\n",
        "\n",
        "    print(f\"[DEBUG] Contoh Lemma: {lemma_tokens[:10]}\")\n",
        "\n",
        "    # 3. BIGRAM FORMATION (Penggabungan Kata)\n",
        "    # Menggunakan statistik PMI (Pointwise Mutual Information)\n",
        "    bigram_measures = BigramAssocMeasures()\n",
        "\n",
        "    # Cari bigram dari token yang SUDAH di-lemma\n",
        "    finder = BigramCollocationFinder.from_words(lemma_tokens)\n",
        "\n",
        "    # Filter: Pasangan kata minimal muncul 2 kali (agar tidak menangkap typo)\n",
        "    finder.apply_freq_filter(2)\n",
        "\n",
        "    # Ambil 50 Bigram terbaik berdasarkan skor PMI\n",
        "    top_bigrams = finder.nbest(bigram_measures.pmi, 100)\n",
        "\n",
        "    # Ubah list bigram menjadi Set agar pencarian cepat\n",
        "    bigram_set = set(top_bigrams)\n",
        "\n",
        "    final_tokens = []\n",
        "    skip_next = False\n",
        "\n",
        "    for i in range(len(lemma_tokens) - 1):\n",
        "        if skip_next:\n",
        "            skip_next = False\n",
        "            continue\n",
        "\n",
        "        word1 = lemma_tokens[i]\n",
        "        word2 = lemma_tokens[i+1]\n",
        "\n",
        "        # Cek apakah pasangan ini ada di daftar Bigram Top\n",
        "        if (word1, word2) in bigram_set:\n",
        "            # GABUNGKAN!\n",
        "            merged_word = f\"{word1}_{word2}\"\n",
        "            final_tokens.append(merged_word)\n",
        "            skip_next = True\n",
        "        else:\n",
        "            # Jika tidak, masukkan sebagai Unigram (kata tunggal)\n",
        "            final_tokens.append(word1)\n",
        "\n",
        "    # Handle kata terakhir\n",
        "    if not skip_next and len(lemma_tokens) > 0:\n",
        "        final_tokens.append(lemma_tokens[-1])\n",
        "\n",
        "    return final_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9ZpkL9ZzUB1",
        "outputId": "92f1be3f-faa5-4b9d-9c56-d22ec3a751c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] Contoh Lemma: ['Abstract', 'disease', 'call', 'Alzheimer', 'always', 'put', 'health', 'agency', 'alarm', 'situation']\n"
          ]
        }
      ],
      "source": [
        "bigram = advanced_token_processing(stopwords)\n",
        "output_final2 = \"hasil_bigram.txt\"\n",
        "with open(output_final2, \"w\", encoding=\"utf-8\") as f:\n",
        "    for item in bigram:\n",
        "        f.write(str(item) + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDzSDEv3zo_y"
      },
      "source": [
        "### Build Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "a5ab5dM5zg5-"
      },
      "outputs": [],
      "source": [
        "def build_graph(tokens, window_size=2):\n",
        "    G = nx.Graph()\n",
        "    # Tambah semua node\n",
        "    for t in tokens:\n",
        "        G.add_node(t)\n",
        "\n",
        "    # Tambah edge berdasarkan sliding window\n",
        "    for i in range(len(tokens) - window_size + 1):\n",
        "        window = tokens[i : i+window_size]\n",
        "        start = window[0]\n",
        "        for end in window[1:]:\n",
        "            if start != end:\n",
        "                if G.has_edge(start, end):\n",
        "                    G[start][end]['weight'] += 1\n",
        "                else:\n",
        "                    G.add_edge(start, end, weight=1)\n",
        "    return G\n",
        "\n",
        "def calculate_all_metrics(G):\n",
        "    print(\"Menghitung metrik sentralitas...\")\n",
        "    # 1. PageRank (Relevansi Global)\n",
        "    pagerank = nx.pagerank(G, weight='weight')\n",
        "\n",
        "    # 2. Degree Centrality (Popularitas Lokal - banyak koneksi langsung)\n",
        "    degree = nx.degree_centrality(G)\n",
        "\n",
        "    # 3. Betweenness Centrality (Jembatan Informasi - penghubung antar kluster)\n",
        "    betweenness = nx.betweenness_centrality(G)\n",
        "\n",
        "    # 4. Closeness Centrality (Pusat Penyebaran - mudah dijangkau)\n",
        "    closeness = nx.closeness_centrality(G)\n",
        "\n",
        "    return pagerank, degree, betweenness, closeness\n",
        "\n",
        "# --- 3. VISUALIZATION ---\n",
        "\n",
        "def visualize_graph(G, pagerank_scores, top_n=40):\n",
        "    # Ambil Top N node berdasarkan PageRank tertinggi untuk divisualisasikan\n",
        "    top_nodes = sorted(pagerank_scores.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
        "    top_keys = [x[0] for x in top_nodes]\n",
        "\n",
        "    # Buat Subgraph (agar gambar tidak terlalu ruwet)\n",
        "    subgraph = G.subgraph(top_keys)\n",
        "\n",
        "    plt.figure(figsize=(12, 12))\n",
        "\n",
        "    # Layout algoritma 'spring' agar node menyebar otomatis secara estetis\n",
        "    pos = nx.spring_layout(subgraph, k=0.5, seed=42)\n",
        "\n",
        "    # Ukuran node berdasarkan skor PageRank (makin penting makin besar)\n",
        "    node_sizes = [pagerank_scores[n] * 10000 for n in subgraph.nodes()]\n",
        "\n",
        "    # Gambar Nodes\n",
        "    nx.draw_networkx_nodes(subgraph, pos, node_size=node_sizes, node_color='#66c2a5', alpha=0.9)\n",
        "\n",
        "    # Gambar Edges (Ketebalan berdasarkan bobot co-occurrence)\n",
        "    weights = [subgraph[u][v]['weight'] for u,v in subgraph.edges()]\n",
        "    max_w = max(weights) if weights else 1\n",
        "    widths = [w/max_w * 3 for w in weights]\n",
        "    nx.draw_networkx_edges(subgraph, pos, width=widths, alpha=0.3, edge_color='gray')\n",
        "\n",
        "    # Gambar Label\n",
        "    nx.draw_networkx_labels(subgraph, pos, font_size=10, font_family=\"sans-serif\", font_weight='bold')\n",
        "\n",
        "    plt.title(f\"Visualisasi Graph - Top {top_n} Keywords\", fontsize=15)\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Simpan ke file\n",
        "    filename = \"keyword_graph.png\"\n",
        "    plt.savefig(filename, format=\"PNG\", dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    return filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjhZWaurzw1F",
        "outputId": "453ef126-a918-45c8-9eb3-937441ca264a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2. Membangun Graph dari 1433 token...\n",
            "Menghitung metrik sentralitas...\n",
            "\n",
            "--- PageRank (Keyword Utama) ---\n",
            "   - disease: 0.0182\n",
            "   - Alzheimers: 0.0160\n",
            "   - VGG: 0.0116\n",
            "   - use: 0.0113\n",
            "   - model: 0.0101\n",
            "   - layer: 0.0075\n",
            "   - data: 0.0068\n",
            "   - Fig: 0.0059\n",
            "   - train: 0.0053\n",
            "   - image: 0.0052\n",
            "\n",
            "--- Degree Centrality (Paling Terhubung) ---\n",
            "   - disease: 0.1270\n",
            "   - Alzheimers: 0.1085\n",
            "   - VGG: 0.0913\n",
            "   - use: 0.0820\n",
            "   - model: 0.0714\n",
            "   - layer: 0.0556\n",
            "   - data: 0.0503\n",
            "   - Fig: 0.0489\n",
            "   - ADNI dataset: 0.0410\n",
            "   - image: 0.0410\n",
            "\n",
            "--- Betweenness Centrality (Jembatan Topik) ---\n",
            "   - disease: 0.2000\n",
            "   - Alzheimers: 0.1559\n",
            "   - VGG: 0.1270\n",
            "   - use: 0.1036\n",
            "   - model: 0.0870\n",
            "   - Fig: 0.0497\n",
            "   - data: 0.0467\n",
            "   - layer: 0.0403\n",
            "   - ADNI dataset: 0.0370\n",
            "   - stage: 0.0352\n",
            "\n",
            "--- Closeness Centrality (Pusat Informasi) ---\n",
            "   - disease: 0.4370\n",
            "   - Alzheimers: 0.4288\n",
            "   - VGG: 0.4257\n",
            "   - model: 0.4104\n",
            "   - use: 0.3838\n",
            "   - Fig: 0.3812\n",
            "   - data: 0.3776\n",
            "   - different: 0.3748\n",
            "   - take: 0.3704\n",
            "   - accuracy: 0.3686\n",
            "\n",
            "3. Membuat Visualisasi Graph...\n",
            "   [SUKSES] Gambar disimpan ke: keyword_graph.png\n"
          ]
        }
      ],
      "source": [
        "print(f\"2. Membangun Graph dari {len(bigram)} token...\")\n",
        "G = build_graph(bigram, window_size=3)\n",
        "# Hitung Metrik\n",
        "pr, deg, bet, clo = calculate_all_metrics(G)\n",
        "\n",
        "# Fungsi Helper untuk print top 10\n",
        "def print_top(title, metric_dict):\n",
        "    print(f\"\\n--- {title} ---\")\n",
        "    sorted_items = sorted(metric_dict.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "    for w, s in sorted_items:\n",
        "        print(f\"   - {w.replace('_', ' ')}: {s:.4f}\")\n",
        "\n",
        "# Tampilkan Hasil Teks\n",
        "print_top(\"PageRank (Keyword Utama)\", pr)\n",
        "print_top(\"Degree Centrality (Paling Terhubung)\", deg)\n",
        "print_top(\"Betweenness Centrality (Jembatan Topik)\", bet)\n",
        "print_top(\"Closeness Centrality (Pusat Informasi)\", clo)\n",
        "\n",
        "# Tampilkan Graph\n",
        "print(\"\\n3. Membuat Visualisasi Graph...\")\n",
        "jumlah_node=len(bigram)\n",
        "# img_path = visualize_graph(G, pr, top_n=jumlah_node)\n",
        "img_path = visualize_graph(G, pr, top_n=20)\n",
        "print(f\"   [SUKSES] Gambar disimpan ke: {img_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAlP0PrJ0DBd",
        "outputId": "cbdd2d1e-fedb-4a9c-84ea-b38a790261b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- PageRank Keywords (Top 20) ---\n",
            "1. disease (0.0182)\n",
            "2. Alzheimers (0.0160)\n",
            "3. VGG (0.0116)\n",
            "4. use (0.0113)\n",
            "5. model (0.0101)\n",
            "6. layer (0.0075)\n",
            "7. data (0.0068)\n",
            "8. Fig (0.0059)\n",
            "9. train (0.0053)\n",
            "10. image (0.0052)\n",
            "11. stage (0.0050)\n",
            "12. ADNI dataset (0.0050)\n",
            "13. patient (0.0049)\n",
            "14. detection (0.0046)\n",
            "15. take (0.0046)\n",
            "16. dataset (0.0043)\n",
            "17. output (0.0041)\n",
            "18. Disease (0.0039)\n",
            "19. feature (0.0039)\n",
            "20. filter (0.0038)\n",
            "\n",
            "--- Degree Centrality Keywords (Top 20) ---\n",
            "1. disease (0.1270)\n",
            "2. Alzheimers (0.1085)\n",
            "3. VGG (0.0913)\n",
            "4. use (0.0820)\n",
            "5. model (0.0714)\n",
            "6. layer (0.0556)\n",
            "7. data (0.0503)\n",
            "8. Fig (0.0489)\n",
            "9. ADNI dataset (0.0410)\n",
            "10. image (0.0410)\n",
            "11. train (0.0397)\n",
            "12. patient (0.0384)\n",
            "13. take (0.0384)\n",
            "14. stage (0.0370)\n",
            "15. feature (0.0357)\n",
            "16. dataset (0.0344)\n",
            "17. detection (0.0331)\n",
            "18. accuracy (0.0317)\n",
            "19. datasets (0.0317)\n",
            "20. class (0.0317)\n"
          ]
        }
      ],
      "source": [
        "def print_top_keywords(metrics_dict, title, n=20):\n",
        "    print(f\"\\n--- {title} (Top {n}) ---\")\n",
        "    # Urutkan dari nilai terbesar ke terkecil\n",
        "    sorted_keywords = sorted(metrics_dict.items(), key=lambda x: x[1], reverse=True)[:n]\n",
        "\n",
        "    for i, (word, score) in enumerate(sorted_keywords, 1):\n",
        "        # Ganti underscore dengan spasi agar bigram terbaca enak (misal: neural_network -> neural network)\n",
        "        readable_word = word.replace('_', ' ')\n",
        "        print(f\"{i}. {readable_word} ({score:.4f})\")\n",
        "\n",
        "# Panggil fungsi\n",
        "print_top_keywords(pr, \"PageRank Keywords\", 20)\n",
        "print_top_keywords(deg, \"Degree Centrality Keywords\", 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2oU7N2bz7JS"
      },
      "source": [
        "## Opsional jika ingin menampilkan semua node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSNlNDj4z98E",
        "outputId": "7091c2df-61c6-466c-c14c-176f1568fc3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sedang merender visualisasi untuk 757 node... (Mungkin butuh waktu)\n"
          ]
        }
      ],
      "source": [
        "def visualize_full_graph(G, pagerank_scores, filename=\"full_graph_viz2.png\"):\n",
        "    \"\"\"\n",
        "    Visualisasi seluruh node dengan strategi 'Sparse Labels' dan 'Heatmap Coloring'\n",
        "    agar tetap terbaca meskipun datanya padat.\n",
        "    \"\"\"\n",
        "    print(f\"Sedang merender visualisasi untuk {len(G.nodes())} node... (Mungkin butuh waktu)\")\n",
        "\n",
        "    plt.figure(figsize=(20, 20)) # Ukuran kanvas ekstra besar\n",
        "\n",
        "    # 1. Layout yang Menyebar (k=0.15 agar tidak terlalu padat)\n",
        "    pos = nx.spring_layout(G, k=0.15, iterations=50, seed=42)\n",
        "\n",
        "    # 2. Persiapan Atribut Visual\n",
        "    # Urutkan node berdasarkan PageRank untuk pewarnaan dan labeling\n",
        "    sorted_nodes = sorted(pagerank_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    top_100_nodes = set([n for n, s in sorted_nodes[:10]]) # Hanya labeli 100 teratas\n",
        "\n",
        "    # List warna dan ukuran sesuai urutan G.nodes()\n",
        "    node_sizes = [pagerank_scores[n] * 8000 + 20 for n in G.nodes()] # +20 agar node kecil tetap kelihatan\n",
        "    node_colors = [pagerank_scores[n] for n in G.nodes()]\n",
        "\n",
        "    # 3. Gambar Nodes (Pakai colormap 'viridis' atau 'plasma')\n",
        "    nodes = nx.draw_networkx_nodes(G, pos,\n",
        "                                   node_size=node_sizes,\n",
        "                                   node_color=node_colors,\n",
        "                                   cmap=plt.cm.plasma,\n",
        "                                   alpha=0.8)\n",
        "\n",
        "    # 4. Gambar Edges (Tipis dan Transparan)\n",
        "    # Hanya gambar edge jika bobotnya cukup berarti untuk mengurangi kekusutan\n",
        "    edges_to_draw = [(u, v) for u, v, d in G.edges(data=True) if d['weight'] > 0]\n",
        "    weights = [G[u][v]['weight'] for u, v in edges_to_draw]\n",
        "    max_w = max(weights) if weights else 1\n",
        "    # Normalisasi ketebalan: garis tipis sekali (0.2) sampai agak tebal (2.0)\n",
        "    widths = [(w / max_w * 30) + 0.1 for w in weights]\n",
        "\n",
        "    nx.draw_networkx_edges(G, pos,\n",
        "                           edgelist=edges_to_draw,\n",
        "                           width=widths,\n",
        "                           alpha=0.15,  # Sangat transparan\n",
        "                           edge_color='#AAAAAA') # Warna abu-abu netral\n",
        "\n",
        "    # 5. Labeling Selektif (Hanya Top 100)\n",
        "    labels = {n: n for n in top_100_nodes}\n",
        "    nx.draw_networkx_labels(G, pos, labels=labels, font_size=8, font_family=\"sans-serif\", font_weight=\"bold\")\n",
        "\n",
        "    # Tambahkan Colorbar sebagai legenda pentingnya node\n",
        "    plt.colorbar(nodes, label=\"PageRank Score\")\n",
        "\n",
        "    plt.title(f\"Full Keyword Co-occurrence Graph ({len(G.nodes())} Nodes)\\nTop 100 Labels Shown\", fontsize=18)\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.savefig(filename, format=\"PNG\", dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    return filename\n",
        "img_path = visualize_full_graph(G, pr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-bR_NxG0IUr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
